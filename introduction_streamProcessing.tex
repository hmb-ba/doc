\chapter{Introduction to Data Streaming}

The view of data as rows of databases or single files changes when one thinks
about what a business actually does with the generated data. Where retail
generates orders they lead to sales, shipments and so on, a financial institution will
generate orders they are going to have an impact of a stock price. Or a social
network platform generates clicks, impressions and searches they are used to
make some sort of intelligent analysis to further display personalized data to
it's users. Such kind of data can be thought of as streams of
events. In fact, collecting all events a business ever generated will lead to the current state
of the business and thus describe what the business did in past. For example the
current price of a stock was generated by all the orders ever made on this
stock. Every order can be captured as an event and so can all events together reproduce
the current stock price.

\section{What is an Event?}
\label{intro-datastream-datastream}
Very basically an event occurs when "something happens"  in a system
like when a user of an online shop adds an item to its basket. In modern systems, events are transmitted as discrete messages on a MOM (see \ref{intro-messaging-mom}) and thus
following Tannenbaum et al. (2006), represent a data unit of a data streams. 
Where a data stream can be applied to discrete as well as continuous media, events are
transmitted as discrete messages only. The message as itself can
be considered as an event message \cite{EIP03}.
\todo[inline]{in the following we'll call messages of a data stream just events}

\section{Processing of events}
The data stream consisting of events as it self is not valuable but
can be taken advantage of by a system that processes the events and produces a
result. This can be the calculation of the new stock price after a customer sold
his stock or the personalized content on my news feed after I subscribed to a new fan
page. But it could also be a more complex analysis over all the collected
event that ever happened, stored in a big database. 
\\ \\
In fact, the above mentioned examples differ in it's nature. Where the
calculation of the stock price is fairly simple by setting the price to the
latest paid stock price without any knowledge about the stock prices in past. 
In contrast, a complete analysis over a huge user base will not only require a
significant amount of processing time, it also requires some data produced in the
past. We distinguish between stream processing for the first and batch
processing for the latter.

\section{Stream Processing}
\label{intro-datastream-streamprocessing}
Stream processing refers to integration and processing of data before storing. 
A stream processing system is built out of multiple units called a processing
element (PE). Each PE receive input from their input queues, does some
computation on the input using its local state and produce output to their
output queues. PE communicate always through messaging with other PEs. 
\\ \\
Most important, those systems are optimized for high latency and high
availability. Recovering from failures is critical for a stream processing
systems and should be fast and efficient. 
Data should partitioned and handled in parallel for large volumes of data. 
The partitioning strategy of a system  affects how the system
handles the data in parallel and how the system can scale. 
\cite{PrpSvyOfDSPS}
\\ \\
Stream processing frameworks---such as Storm, Samza, or Spark
Streaming---were especially developed to provide rich processing primitives and thus can be taken advantage of
in the data integration- and processing stages.


\section{Batch Processing}
\label{intro-datastream-batchprocessing}
Traditional batch processing systems nowadays are distinguished between
map-reduce based and non map-reduce \todo[inline]{ref to literature or glossary} based systems and typically consists of two
stages, data integration and data analytics. 
\\ \\
The process of data extraction-transformation-load (ETL  \todo[inline]{glossary}), 
faced in the data integration
stage, runs at a regular time interval, such as daily, weekly or monthly. 
\\ \\
Analyzing data that resides in a data store is being faced in the
data analytics stage and becomes challenging when data size grows and systems
may not be able to process results within a time limit.
\cite{Liu:2014:SRP:2628194.2628251}


\section{Real-time Batch Processing}

As the trend shows, the needs of performance and responsiveness in a big data environment can't be fulfilled with 
traditional batch processing anymore. Instead, real-time processing becomes more 
important than ever to achieve results from queries in minutes, even seconds. 
\cite{bange2013big}

\todo[inline]{LW: Mir fehlt irgendwie noch die Erklärung was traditional batch
processing ist und warum es den heutigen anforderungen nicht mehr gerecht
werden kann (eingesetzt wird es ja nach wie vor) }

In real-time processing fashion, systems will address the data integration stage
with continual input of data. Processing in near-real-time [glossar] to present 
results within seconds is being addressed in the data analytics stage. Thus,
real-time processing gives organization the ability to take immediate action
for those times when acting within seconds or minutes is significant.
\cite{PrpSvyOfDSPS}

\todo[inline]{LW: Rein vom Text habe ich das mit den Stages irgendwie nicht ganz
verstanden. Du schreibst batch processing beinhaltet diese beiden Stages. Aber
Real-time scheinbar auch? Sind das die gleichen Stages?  }


\subsection{Lambda Architecture}
The lambda architecture introduces a new paradigm for big data which allows
processing of massive data volumes in near real-time fashion and thus results within
seconds can be achieved. 
\\
Any query is answered through the serving layer by querying 
both the speed and the batch layer. Where the badge layer computes views on the current collected data and
is being outdated at the end of it's computation, the speed layer closes this 
gap by constantly processing the most recent data in near real-time fashion. 
\cite{marz2015big} \cite{PrpSvyOfDSPS}

\section{Stream Broker}
Any system which is dependent on a continuous input of data requires a delivery
system that can provide data constantly to it's consumers. Stream processing
systems, whether being in a lambda architecture or not, obviously holds this requirement
as well as it's especially dependent on low-latency.

But if one thinks more traditionally, even database systems can be thought of as
an even stream. The process of creating a backup in form of dumps won't scale as
we increase the frequency of dumps over time. Not only will the process take
longer according to the size of the database, also system resources are limited
during this process. Another approach to make this more efficient is change
capture, which describes a single row changes made on a database. If this can be
done continuously a continuous sequence of single row change is what is being
left. This in in fact, can again be described as a stream of events resulting in a
data stream (\ref{intro-datastream-datastream}).

On the other hand, in a big data environment there is also the requirement of
batch processing systems (\ref{intro-datastream-batchprocessing}) being served
with data. In a lambda environment this could be done again using a stream
processing framework (\ref{intro-datastream-streamprocessing}) responsible for
serving the batch processing system with integrated data, ready for data
analysis. However an other way of doing so would be a data store---such as the
hadoop file system (HDFS)---where data can be directly take for further analysis.
\todo[inline]{hdfs glossary}

The same requirement of a data store holds for any other business intelligence
system followed by the problem of the dependency of a data store which can
not---due to the lack of an adapter---being served with data by a stream processing 
system as comfortable as the HDFS.

- problem of traditional messaging systems
- point to the need of data stream broker systems

%todo: point to the need of messaging systems that deliver data continously ->
%stream source 

-From traditional PCs and Smartphones to a lot of sensors who are connected to
the interne -> Internet of Things!

\todo[inline]{Einbringen des folgenden Statements: The nice thing about this
architecture is that you can now have multiple consumers for the same event
data. You can have one consumer which simply archives the raw events to some big
storage; even if you don’t yet have the capability to process the raw events,
you might as well store them, since storage is cheap and you can use them in
future. Then you can have another consumer which does some aggregation (for
example, incrementing counters), and another consumer which does something else.
Those can all feed off the same event stream.}

\section{Complex event processing (CEP)}
In literatur there is often a confusion about the difference between the terms
complex event processing and stream event processing. Both systems work on
events and produce results based on the properties of the events... 
- Zitat: Combines data from multiple sources  to detect patterns and attempt to
identify either opportunities or threats. The goal is to identify significant
events and respond fast. Sales leads, orders or customer service calls are
examples.\\

\todo[inline]{move to glossary?}

